# flume configuration for log trapping
This conf listens to the hbase RS log and sends it to a kafka channel.
There is no sink in this example.  This is cool because the consumer
can be anybody.

# kafka-topics --zookeeper hddev1mb004dxc1.dev.oclc.org:2181/kafka --create --topic dbahadoop.log.collection --partitions 1 --replication-factor 2
#
#
# HBase log file source
a1.sources.app.type =  TAILDIR
a1.sources.app.channels = ch1
a1.sources.app.filegroups = f1
a1.sources.app.filegroups.f1 = /var/log/hbase/hbase-cmf-hbase2-REGIONSERVER-.*.dev.oclc.org.log.out

# Kafka Channel
a1.channels.ch1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.ch1.kafka.bootstrap.servers = hddev1mb004dxc1.dev.oclc.org:9092,hddev1mb005dxc1.dev.oclc.org:9092,hddev1mb006dxc1.dev.oclc.org:9092
a1.channels.ch1.kafka.topic = dbahadoop.log.collection
a1.channels.ch1.kafka.consumer.group.id = dbahadoop
a1.channels.ch1.kafka.parseAsFlumeEvent = false
a1.channels.ch1.kafka.consumer.auto.offset.reset = latest

# main
a1.sources = app
a1.channels = ch1
